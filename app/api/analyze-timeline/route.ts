import { NextRequest, NextResponse } from 'next/server'
import { TimelineAnalyzer, ProgressUpdate } from '@/lib/timeline-analyzer'

// Disable timeout for this route (audio analysis can take 30+ minutes)
export const maxDuration = 300 // 5 minutes for Vercel free tier, or set higher for paid plans
export const dynamic = 'force-dynamic'

export async function POST(request: NextRequest) {
  try {
    const { url, interests, model = 'gemini', useAudioAnalyzer = false } = await request.json()

    if (!url || !interests) {
      return NextResponse.json(
        { success: false, error: 'URL and interests are required' },
        { status: 400 }
      )
    }

    console.log(`Starting timeline analysis for URL: ${url}`)
    console.log(`User interests: ${interests}`)
    console.log(`Selected model: ${model}`)
    console.log(`Audio analyzer enabled: ${useAudioAnalyzer}`)

    // Create analyzer with progress callback
    const progressUpdates: ProgressUpdate[] = []
    const analyzer = new TimelineAnalyzer((update: ProgressUpdate) => {
      progressUpdates.push(update)
      console.log(`Progress Update: ${update.step} - ${update.progress}% - ${update.message}`)
    }, model)
    
    // Step 1: Analyze timeline and get relevant segments
    const timelineResult = await analyzer.analyzeVideo(url, interests, useAudioAnalyzer)
    
    console.log(`Found ${timelineResult.relevantSegments.length} relevant segments`)
    console.log(`Analysis source: ${timelineResult.source}`)
    
    let recommendations: any[]
    let audioPaths: string[] = []
    let transcripts: any[] = []
    
    // Check if AudioAnalyzer was used - if so, use its data directly without re-processing
    if (timelineResult.source === 'assemblyai_python' && timelineResult.audioAnalyzerData) {
      console.log('Using AudioAnalyzer data with CORRECT TIMINGS for video playback')
      
      // Use AudioAnalyzer segments directly - they already have correct timestamps
      recommendations = timelineResult.audioAnalyzerData.segments.map((audioSegment: any) => {
        // Match with relevant segment if it was filtered by LLM
        const relevantSeg = timelineResult.relevantSegments.find((seg: any) =>
          seg.title === audioSegment.title || seg.startTime === audioSegment.startTime
        )
        
        return {
          startTime: audioSegment.startTime,   // "0:15" - from AudioAnalyzer
          endTime: audioSegment.endTime,        // "2:30" - from AudioAnalyzer  
          summary: audioSegment.title || audioSegment.description?.slice(0, 100) || 'Audio segment',
          relevanceScore: relevantSeg?.relevanceScore || 90,
          topics: audioSegment.topics || [],
          reasoning: relevantSeg?.reasoning || audioSegment.description || 'Segment generated by AssemblyAI audio analysis',
          transcript: audioSegment.description || 'Transcript not available',
          transcriptSummary: audioSegment.description || '',
          confidence: audioSegment.confidence || 0.95,
          sentiment: audioSegment.sentiment || 'neutral',
          startMs: audioSegment.startMs,
          endMs: audioSegment.endMs,
          source: 'assemblyai_python'
        }
      })
      
      console.log(`Created ${recommendations.length} recommendations with timings:`, 
        recommendations.map((r: any) => `${r.startTime}-${r.endTime}`))
      
    } else {
      // YouTube timeline segments - extract and transcribe audio
      console.log('Extracting relevant audio chunks...')
      audioPaths = await analyzer.extractRelevantAudioChunks(url, timelineResult.relevantSegments)
      
      console.log('Transcribing relevant audio chunks...')
      transcripts = await analyzer.transcribeAudioChunks(audioPaths)
      
      // Combine results with transcriptions
      recommendations = timelineResult.relevantSegments.map((segment, index) => ({
        startTime: segment.startTime,
        endTime: segment.endTime,
        summary: segment.title,
        relevanceScore: segment.relevanceScore,
        topics: segment.topics,
        reasoning: segment.reasoning,
        transcript: transcripts[index]?.text || 'Transcription not available',
        transcriptSummary: transcripts[index]?.summary || 'Analysis summary not available',
        confidence: transcripts[index]?.confidence || 0.95,
        auto_highlights_result: transcripts[index]?.auto_highlights_result,
        sentiment_analysis_results: transcripts[index]?.sentiment_analysis_results,
        entities: transcripts[index]?.entities,
        iab_categories_result: transcripts[index]?.iab_categories_result,
        auto_chapters_result: transcripts[index]?.auto_chapters_result,
        source: 'youtube'
      }))
    }

    // Cleanup only if audio files were created
    if (audioPaths.length > 0) {
      await analyzer.cleanup(audioPaths)
    }

    return NextResponse.json({
      success: true,
      videoInfo: timelineResult.videoInfo,
      recommendations,
      analysisDetails: {
        totalSegments: timelineResult.segments.length,
        relevantSegments: timelineResult.relevantSegments.length,
        audioChunksExtracted: audioPaths.length,
        transcriptsGenerated: transcripts.length,
        processingTime: timelineResult.processingTime,
        userInterests: timelineResult.userInterests,
        extractedKeywords: timelineResult.extractedKeywords,
        timelineAnalysis: timelineResult.segments.map(seg => ({
          time: seg.startTime,
          title: seg.title,
          relevanceScore: seg.relevanceScore,
          reasoning: seg.reasoning,
          topics: seg.topics
        })),
        progressUpdates: progressUpdates
      },
      steps: [
        {
          name: 'keyword_extraction',
          progress: 100,
          status: 'completed',
          message: 'Keywords extracted from user interests',
          details: `Keywords: ${timelineResult.extractedKeywords.join(', ')}`
        },
        {
          name: 'extract_video_info',
          progress: 100,
          status: 'completed',
          message: 'Video information extracted',
          details: `Title: ${timelineResult.videoInfo.title} | Duration: ${timelineResult.videoInfo.duration}`
        },
        {
          name: 'parse_timeline',
          progress: 100,
          status: 'completed',
          message: timelineResult.source === 'assemblyai_python' 
            ? 'Segments generated using AssemblyAI audio analysis'
            : timelineResult.source === 'fallback'
            ? 'Using fallback single segment'
            : 'Timeline parsed from description',
          details: `Found ${timelineResult.segments.length} timeline segments (Source: ${timelineResult.source})`
        },
        {
          name: 'analyze_relevance',
          progress: 100,
          status: 'completed',
          message: `Relevance analysis completed with ${model === 'gemini' ? 'Gemini' : 'OpenAI GPT'}`,
          details: `${timelineResult.relevantSegments.length} highly relevant segments identified`
        },
        {
          name: 'filter_segments',
          progress: 100,
          status: 'completed',
          message: 'Most relevant segments filtered',
          details: `Selected top ${timelineResult.relevantSegments.length} segments for processing`
        },
        ...(timelineResult.source === 'assemblyai_python' ? [] : [
          {
            name: 'extract_audio_chunks',
            progress: 100,
            status: 'completed',
            message: 'Relevant audio chunks extracted',
            details: `Extracted ${audioPaths.length} audio chunks`
          },
          {
            name: 'transcribe_chunks',
            progress: 100,
            status: 'completed',
            message: 'Audio chunks transcribed',
            details: `Generated ${transcripts.length} transcripts`
          }
        ])
      ]
    })

  } catch (error: any) {
    console.error('Timeline analysis error:', error)
    console.error('Error stack:', error.stack)
    console.error('Error details:', {
      message: error.message,
      name: error.name,
      cause: error.cause
    })
    
    return NextResponse.json(
      { 
        success: false, 
        error: error.message || 'Timeline analysis failed',
        errorDetails: {
          message: error.message,
          stack: error.stack,
          name: error.name
        },
        steps: [
          {
            name: 'error',
            progress: 100,
            status: 'error',
            message: `Analysis failed: ${error.message || 'Unknown error'}`,
            details: 'Check server logs for more details'
          }
        ]
      },
      { status: 500 }
    )
  }
}
